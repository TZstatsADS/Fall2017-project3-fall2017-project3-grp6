---
title: "Project 3 - Example Main Script"
author: "Yuting Ma, Tian Zheng"
date: "February 24, 2016"
output:
  pdf_document: default
  html_document: default
---
#Install necessary packages.
```{r}
if(!require("EBImage")){
  source("https://bioconductor.org/biocLite.R")
  biocLite("EBImage")
}

packages.used=c("gbm", "caret","DMwR" ,"nnet","randomForest","e1071")
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

library("EBImage")
library("gbm")
library("caret")
library("DMwR")
library("nnet")
library("randomForest")
library("e1071")
```
#Set the working directory to the image folder.
```{r}
setwd("/Users/sijian/Documents/Github/Fall2017-project3-fall2017-project3-grp6/doc") 
# here replace it with your own path or manually set it in RStudio to where this rmd file is located.
```
#Read Data
```{r,warning=F,message=F}
sift.feature=read.csv("../data/sift_feature.csv", header = T)
lbp.feature=read.csv("../data/lbp_feature.csv", header = F)
hog.feature = read.csv("../data/hog_feature.csv")
label=read.csv("../data/trainlabel.csv")
```
#Train and Validate set 
```{r}
sift_data=data.frame(cbind(label,sift.feature[,-1]))
test.index=sample(1:3000,500,replace=F)
colnames(sift_data)[2]="y"
sift_data = sift_data[,-1]
test.sift=sift_data[test.index,]
test.x.sift=test.sift[,-1]
train.sift=sift_data[-test.index,]
```
#Baseline model: GBM + SIFT
#####Tune parameters:n.trees = 250, shrinkage = 0.1
```{r}
#y<-label[,2]
#X<-test.x.sift
#source("../lib/tune gbm.r")
# These lines of code takes a long like crazy time so I just wrote the results as a csv file.
#for(k in 1:length(depths)){
#  cat("k=", k, "\n")
#  err_cv[k,] <- cv.function(X, y, depths[k], shrinkage=shrinkages, K=5)  #K=5
#}
#colnames(err_cv) = c("mean of cv.error","sd of cv.error")
#rownames(err_cv) = c("depth = 3", "depth = 5", "depth = 7", "depth = 9","depth = 11")
#print(err_cv)
#write.csv(err_cv,file = "err_cv_for_baseline.csv")
```
#Other models + SIFT
####The 5000-dimensional SIFT feature takes a long long long time to get the results. If PCA is used to do dimension reduction, the accuracy become really low. It makes sense because doing PCA dimension reduction means losing information. So we started to use other feature extraction methods. With Zhilin's suggestion, we use Local Binary Patterns(LBP), Histogram of oriented gradients(HoG)
and Convolutional Neural Network(CNN) to extract features.
#Local Binary Patterns(LBP)
####Local Binary Pattern (LBP) is a simple yet very efficient texture operator which labels the pixels of an image by thresholding the neighborhood of each pixel and considers the result as a binary number. Due to its discriminative power and computational simplicity, LBP texture operator has become a popular approach in various applications. It can be seen as a unifying approach to the traditionally divergent statistical and structural models of texture analysis. Perhaps the most important property of the LBP operator in real-world applications is its robustness to monotonic gray-scale changes caused, for example, by illumination variations. Another important property is its computational simplicity, which makes it possible to analyze images in challenging real-time settings.
####A useful extension to the original operator is the so-called uniform pattern, which can be used to reduce the length of the feature vector and implement a simple rotation invariant descriptor. This idea is motivated by the fact that some binary patterns occur more commonly in texture images than others. A local binary pattern is called uniform if the binary pattern contains at most two 0-1 or 1-0 transitions. For example, 00010000(2 transitions) is a uniform pattern, 01010100(6 transitions) is not. In the computation of the LBP histogram, the histogram has a separate bin for every uniform pattern, and all non-uniform patterns are assigned to a single bin. Using uniform patterns, the length of the feature vector for a single cell reduces from 256 to 59. The 58 uniform binary patterns correspond to the integers 0, 1, 2, 3, 4, 6, 7, 8, 12, 14, 15, 16, 24, 28, 30, 31, 32, 48, 56, 60, 62, 63, 64, 96, 112, 120, 124, 126, 127, 128, 129, 131, 135, 143, 159, 191, 192, 193, 195, 199, 207, 223, 224, 225, 227, 231, 239, 240, 241, 243, 247, 248, 249, 251, 252, 253, 254 and 255.
####We used MATLAB to extract LBP features(adapted codes from Zhilin's work, I added a filter for color image and grayscale image). The column dimension is 59, which is much less than 5000. So it is reasonable that we expect a decreased time usage.
```{r}
source("../lib/train.r")
source("../lib/test.r")

lbpdata = data.frame(cbind(label,lbp.feature))
colnames(lbpdata)[2] = "y"
lbpdata = lbpdata[,-1]
test.lbp = lbpdata[test.index,]
test.x.lbp = test.lbp[,-1]
train.lbp = lbpdata[-test.index,]
```
#GBM + LBP
```{r}
X = train.lbp[,-1]
y = train.lbp[,1]
source("../lib/tune gbm.r")
for(k in 1:length(depths)){
  cat("k=", k, "\n")
  err_cv[k,] <- cv.function(X, y, depths[k], shrinkage=shrinkages, K=5)  #K=5
}
colnames(err_cv) = c("mean of cv.error","sd of cv.error")
rownames(err_cv) = c("depth = 3", "depth = 5", "depth = 7", "depth = 9","depth = 11")
print(err_cv)
write.csv(err_cv,file = "err_cv_for_GBM+LBP.csv")
```
#Some advanced models + LBP
###BPNN
####Tune parameters:size = 1, decay = 0.01
```{r}
bp.model=train.bp(train.lbp)
bp.pre=test.bp(bp.model,test.x.lbp)
table(bp.pre,test.lbp$y)
```
##Random Forest + LBP 
####Tune Parameter: m.try=15
```{r}
rf.model <- train.rf(train.lbp)
rf.pre=test.rf(rf.model,test.x.lbp)
table(rf.pre,test.lbp$y)
```
##SVM + LBP ####Tune Parameters: cost=10, gamma=0.01
```{r}
svm.model <- train.svm(train.lbp)
svm.pre=test.svm(svm.model,test.x.lbp)
table(svm.pre,test.lbp$y)
```
##Logistic Regression + LBP
```{r}
log.model <- train.log(train.lbp)
log.pre=test.log(log.model, test.x.lbp)
P
table(log.pre, test.lbp$y)
```
##Majority Vote (NN, SVM, Log) + LBP
```{r}
pre=(as.numeric(as.character(bp.pre))+as.numeric(as.character(log.pre))+as.numeric(as.character(svm.pre)))
pre=ifelse(pre>=2,1,0)
table(pre,test$y)
```
#Cross Validation Error Rate
```{r}
source("../lib/cross_validation.R")
cv.error=cv.function(train, 5)
#Final Train & Time

a=system.time(baseline <- train.baseline(all))
b=system.time(gbm <- train.baseline(new.data))
c=system.time(bp <- train.bp(new.data))
d=system.time(rf <- train.rf(new.data))
e=system.time(svm <- train.svm(new.data))
f=system.time(logistic <- train.log(new.data))
time=list(baseline=a,gbm=b,bp=c,rf=d,svm=e,logistic=f,vote=NA)
cv.error
time
#Final Model ####We choose Majority Vote as our final model. Since training time of each model is very short, time won't be a problem for majority vote. Although we sacrifice little accuracy, We can get a more robust model.

